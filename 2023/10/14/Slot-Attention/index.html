<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>

    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <!--Description-->

    

    
        <meta name="description" content="记录Slot-Attention学习中遇到的一些问题。阅读源码中发现需要补充的Pytorch知识：torch.einsum()用于执行爱因斯坦求和约定(Einstein summation convention)，是一种用于表示多维张量乘法和求和的紧凑符号表示法。爱因斯坦求和约定是由物理学家阿尔伯特"/>
    

    <!--Author-->
    
        <meta name="author" content="Shixuan Wu"/>
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Slot Attention"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="记录Slot-Attention学习中遇到的一些问题。阅读源码中发现需要补充的Pytorch知识：torch.einsum()用于执行爱因斯坦求和约定(Einstein summation convention)，是一种用于表示多维张量乘法和求和的紧凑符号表示法。爱因斯坦求和约定是由物理学家阿尔伯特"/>
    

    <!--Open Graph Site Name-->
        <meta property="og:site_name" content="metikos"/>

    <!--Type page-->
    
        <meta property="og:type" content="article"/>
    

    <!--Page Cover-->
    
    
        <meta property="og:image" content="http://example.comimg/home-my.jpg"/>
    

        <meta name="twitter:card" content="summary_large_image"/>

    

    
        <meta name="twitter:image" content="http://example.comimg/home-my.jpg"/>
    

    <!-- Title -->
    
    <title>Slot Attention - metikos</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"/>
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"/>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet"/>

    <!-- Google Analytics -->
    


    <!-- favicon -->
    

<meta name="generator" content="Hexo 6.3.0"></head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Shixuanwu</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a target="_blank" rel="noopener" href="https://github.com/panyujun123">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/home-my.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Slot Attention</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2023-10-14
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <h3 id="记录Slot-Attention学习中遇到的一些问题。"><a href="#记录Slot-Attention学习中遇到的一些问题。" class="headerlink" title="记录Slot-Attention学习中遇到的一些问题。"></a><strong>记录Slot-Attention学习中遇到的一些问题。</strong></h3><h4 id="阅读源码中发现需要补充的Pytorch知识："><a href="#阅读源码中发现需要补充的Pytorch知识：" class="headerlink" title="阅读源码中发现需要补充的Pytorch知识："></a>阅读源码中发现需要补充的Pytorch知识：</h4><h5 id="torch-einsum"><a href="#torch-einsum" class="headerlink" title="torch.einsum()"></a><strong>torch.einsum()</strong></h5><p>用于执行爱因斯坦求和约定(Einstein summation convention)，是一种用于表示多维张量乘法和求和的紧凑符号表示法。爱因斯坦求和约定是由物理学家阿尔伯特·爱因斯坦引入的。爱因斯坦在他的相对论和引力理论的研究中，为了简化张量运算的书写，提出了一种紧凑而强大的数学表示法，这就是爱因斯坦求和约定。可实现包括但不限于：向量内积，向量外积，矩阵乘法，转置和张量收缩（tensor contraction）等张量操作，熟练运用 einsum 可以很方便的实现复杂的张量操作，而且不容易出错。语法为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(equation, *operands) -&gt; Tensor</span><br></pre></td></tr></table></figure>

<p>参数：</p>
<ul>
<li><p><strong>equation</strong> (str)  - The subscripts for the Einstein summation(爱因斯坦求和的下标)</p>
</li>
<li><p><strong>operands</strong> (<em>List</em><em>[*<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a></em>]*) – The tensors to compute the Einstein summation of(用于计算爱因斯坦求和的张量)</p>
</li>
<li><p><strong>Return type</strong>:   Tensor</p>
</li>
</ul>
<h4 id="Slot-Attention-code"><a href="#Slot-Attention-code" class="headerlink" title="Slot-Attention code:"></a><strong>Slot-Attention code:</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SlotAttention</span>(nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化模型的参数，包括槽的数量、维度、迭代次数,防止除零的小常数eps</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_slots, dim, iters=<span class="number">3</span>, eps=<span class="number">1e-8</span>, hidden_dim=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_slots = num_slots</span><br><span class="line">        self.iters = iters</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.scale = dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化slots的均值和对数标准差</span></span><br><span class="line">        slots_mu = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        slots_sigma = nn.Parameter(<span class="built_in">abs</span>(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim)))</span><br><span class="line">        mu = slots_mu.expand(<span class="number">1</span>, self.num_slots, -<span class="number">1</span>)</span><br><span class="line">        sigma = slots_sigma.expand(<span class="number">1</span>, self.num_slots, -<span class="number">1</span>)</span><br><span class="line">        self.initial_slots = nn.Parameter(torch.normal(mu, sigma))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建用于将slot转化为查询Q的模块</span></span><br><span class="line">        self.to_q = nn.Linear(dim, dim)</span><br><span class="line">        <span class="comment"># 创建用于将input转换成key的模块</span></span><br><span class="line">        self.to_k = nn.Linear(dim, dim)</span><br><span class="line">        <span class="comment"># 创建用于将input转换成value的模块</span></span><br><span class="line">        self.to_v = nn.Linear(dim, dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># GRUCell用于更新slots</span></span><br><span class="line">        self.gru = nn.GRUCell(dim, dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 多层感知机（MLP），用于更新 slots</span></span><br><span class="line">        hidden_dim = <span class="built_in">max</span>(dim, hidden_dim)</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(hidden_dim, dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># LayerNorm 用于归一化输入、slots 和预 MLP 输出</span></span><br><span class="line">        self.norm_input = nn.LayerNorm(dim)</span><br><span class="line">        self.norm_slots = nn.LayerNorm(dim)</span><br><span class="line">        self.norm_pre_ff = nn.LayerNorm(dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs,num_slots=<span class="literal">None</span></span>):</span><br><span class="line">        b, n, d = inputs.shape</span><br><span class="line">        slots = self.initial_slots.expand(b, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        n_s = num_slots <span class="keyword">if</span> num_slots <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.num_slots</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对输入进行归一化</span></span><br><span class="line">        inputs = self.norm_input(inputs)</span><br><span class="line">        k, v = self.to_k(inputs), self.to_v(inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.iters):</span><br><span class="line">            slots_prev = slots</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对 slots 进行归一化</span></span><br><span class="line">            slots = self.norm_slots(slots)</span><br><span class="line">            q = self.to_q(slots)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算点乘得分，使用 einsum 函数</span></span><br><span class="line">            dots = torch.einsum(<span class="string">&#x27;bid,bjd-&gt;bij&#x27;</span>, q, k) * self.scale</span><br><span class="line">            attn = dots.softmax(dim=<span class="number">1</span>) + self.eps</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 归一化注意力分布</span></span><br><span class="line">            attn = attn / attn.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># 使用注意力分布对值进行加权求和，得到更新</span></span><br><span class="line">            updates = torch.einsum(<span class="string">&#x27;bjd,bij-&gt;bid&#x27;</span>, v, attn)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用 GRUCell 更新 slots</span></span><br><span class="line">            slots = self.gru(</span><br><span class="line">                updates.reshape(-<span class="number">1</span>, d),</span><br><span class="line">                slots_prev.reshape(-<span class="number">1</span>, d)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 将结果重新形状为原始的形状，并通过 MLP 进行更新</span></span><br><span class="line">            slots = slots.reshape(b, -<span class="number">1</span>, d)</span><br><span class="line">            slots = slots + self.mlp(self.norm_pre_ff(slots))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slots</span><br></pre></td></tr></table></figure>

<p>运行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 创建 Slot Attention 模型</span></span><br><span class="line">    num_slots = <span class="number">4</span></span><br><span class="line">    dim = <span class="number">256</span></span><br><span class="line">    iters = <span class="number">3</span></span><br><span class="line">    eps = <span class="number">1e-8</span></span><br><span class="line">    hidden_dim = <span class="number">128</span></span><br><span class="line">    slot_attention_model = SlotAttention(num_slots, dim, iters, eps, hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建输入张量</span></span><br><span class="line">    input_tensor = torch.randn(<span class="number">2</span>, <span class="number">10</span>, dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 运行模型</span></span><br><span class="line">    output_slots = slot_attention_model(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出 slots 的形状</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Output Slots Shape:&quot;</span>, output_slots.shape)</span><br></pre></td></tr></table></figure>



                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    
                        <li>
                            <a href="https://github.com/panyujun123" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2023 Shixuan Wu<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>